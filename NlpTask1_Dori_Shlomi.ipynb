{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9UOd7DVA3f/2nfpblhpPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorirozen/AI_Tasks/blob/main/NlpTask1_Dori_Shlomi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:  \n",
        "in each place that we are printing something , we only printed a slice of [0:5] for not expanding the output cell result.."
      ],
      "metadata": {
        "id": "pvcR4K1RwfI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pips\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install beautifulsoup4\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LI0ArK3jeYRe",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports\n",
        "import nltk,time,spacy,csv,requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "from statistics import mean\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk import pos_tag"
      ],
      "metadata": {
        "id": "tO06CE1cebPa",
        "cellView": "form"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the necessary NLTK resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "F1SFN1TiirSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title constant\n",
        "NLP = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y6Wi4NcclxkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Global Functions - printListsAfterActivation & token_stem_limmatizeing & analyze_array_data & csvfile & spam2text & word_freq & print_text_statistics Functions\n",
        "def analyze_array_data(arr):\n",
        "    '''\n",
        "    print_text_statistics for list of lists\n",
        "    '''\n",
        "    all_words = sum(len(sub_array) for sub_array in arr)\n",
        "    word_frequencies = Counter(word for sub_array in arr for word in sub_array)\n",
        "    average_count = sum(len(sub_array) for sub_array in arr) / len(arr) if arr else 0\n",
        "\n",
        "    most_common_words = word_frequencies.most_common(5)\n",
        "    unique_words = sum(1 for count in word_frequencies.values() if count == 1)\n",
        "\n",
        "    print(f\"Total words: {all_words}\")\n",
        "    # print(f\"Word frequencies: {word_frequencies}\")\n",
        "    print(f\"Average words per message: {average_count:.2f}\")\n",
        "    print(f\"Most common words: {', '.join(word for word, _ in most_common_words)}\")\n",
        "    print(f\"Unique words appearing only once: {unique_words}\")\n",
        "\n",
        "def printListsAfterActivation(text):\n",
        "  tokenized_sentences = [word_tokenize(sentence) for sentence in text]\n",
        "\n",
        "  lemmatized_sentences = []\n",
        "  stemmed_sentences = []\n",
        "\n",
        "  for sentence in tokenized_sentences:\n",
        "    lemmatized_sentence = []\n",
        "    stemmed_sentence = []\n",
        "\n",
        "\n",
        "    doc = NLP(' '.join(sentence))\n",
        "    lemmatized_sentence = [token.lemma_ for token in doc]\n",
        "\n",
        "\n",
        "    stemmed_sentence = [stemmer.stem(token) for token in sentence]\n",
        "\n",
        "    lemmatized_sentences.append(lemmatized_sentence)\n",
        "    stemmed_sentences.append(stemmed_sentence)\n",
        "\n",
        "\n",
        "  print(\"\\nTokenized Sentences:\")\n",
        "  print(tokenized_sentences[0:5])\n",
        "\n",
        "  print(\"\\nLemmatized Sentences:\")\n",
        "  print(lemmatized_sentences[0:5])\n",
        "\n",
        "  print(\"\\nStemmed Sentences:\")\n",
        "  print(stemmed_sentences[0:5])\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def token_stem_limmatizeing(text):\n",
        "  tokenized_sentences = [word_tokenize(sentence) for sentence in text]\n",
        "  lemmatized_sentences = []\n",
        "  stemmed_sentences = []\n",
        "\n",
        "  for sentence in tokenized_sentences:\n",
        "    doc = NLP(' '.join(sentence))\n",
        "    lemmatized_sentence = [token.lemma_ for token in doc]\n",
        "\n",
        "    stemmed_sentence = [stemmer.stem(token) for token in sentence]\n",
        "\n",
        "    lemmatized_sentences.append(lemmatized_sentence)\n",
        "    stemmed_sentences.append(stemmed_sentence)\n",
        "\n",
        "  print(\"Original Text Statistics:\")\n",
        "  analyze_array_data(tokenized_sentences)\n",
        "\n",
        "  print(\"\\nLemmatized Text Statistics:\")\n",
        "  analyze_array_data(lemmatized_sentences)\n",
        "\n",
        "  print(\"\\nStemmed Text Statistics:\")\n",
        "  analyze_array_data(stemmed_sentences)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def csvfile(namefile):\n",
        "    if \".csv\" not in namefile:\n",
        "        namefile += \".csv\"\n",
        "    csvfile = open(namefile, 'r', newline='', encoding = \"ISO-8859-1\")\n",
        "    return list(csv.DictReader(csvfile))\n",
        "\n",
        "\n",
        "def spam2text(spam):\n",
        "  return [x['v2'] for x in spam]\n",
        "\n",
        "def word_freq(text):\n",
        "  word_dict = {}\n",
        "  for string in text:\n",
        "    for word in string.split(' '):\n",
        "      try:\n",
        "        word_dict[word] +=1\n",
        "      except KeyError:\n",
        "        word_dict[word] = 1\n",
        "\n",
        "  return dict(sorted(word_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "def print_text_statistics(text):\n",
        "  text_freq = word_freq(text)\n",
        "  print(f\"Word Count: {sum([len(x.split(' ')) for x in text])}\\n\" +\n",
        "        f\"Average Word Count: {mean([len(x.split(' ')) for x in text])}\\n\" +\n",
        "        f\"5 Most Frequent Words : {[freq for freq in text_freq.keys()][0:5]}\\n\" +\n",
        "        f\"Number of Rare Words : {len([freq for freq in text_freq.values() if freq==1])}\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ROUim1AGjd7J"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Starts program here\n",
        "spam = csvfile('/content/spam.csv')\n",
        "print(f\"Number of SMS Messages:{len(spam)}\\n\" +\n",
        "      f\"Number of Spams: {len([x for x in spam if x['v1']=='spam'])}\\n\"+\n",
        "      f\"Number of hams: {len([x for x in spam if x['v1']=='ham'])}\\n\")\n",
        "spam_text = spam2text(spam)\n",
        "print_text_statistics(spam_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "6LlFKtf-juxE",
        "outputId": "7392c568-bb14-4840-ad06-56dcefb57ba0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of SMS Messages:5572\n",
            "Number of Spams: 747\n",
            "Number of hams: 4825\n",
            "\n",
            "Word Count: 86961\n",
            "Average Word Count: 15.60678391959799\n",
            "5 Most Frequent Words : ['to', 'you', 'I', 'a', 'the']\n",
            "Number of Rare Words : 9270\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title tokenize with nltk and spacy\n",
        "# Tokenize the SMS text using NLTK\n",
        "start_time = time.perf_counter()\n",
        "tokenized_sms_nltk = [word_tokenize(sms) for sms in spam_text]\n",
        "end_time = time.perf_counter()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Nltk Execution time: {execution_time:.2f} seconds\\n\\n\")\n",
        "\n",
        "print(\"NLTK Tokenization:\\n\\n\")\n",
        "sms_nltk = lambda i: print(tokenized_sms_nltk[i])\n",
        "for i in range(4):\n",
        "    sms_nltk(i)\n",
        "\n",
        "# Tokenize the SMS text using Spacy\n",
        "start_time = time.perf_counter()\n",
        "tokenized_sms_spacy = [[token.text for token in NLP(sms)] for sms in spam_text]\n",
        "end_time = time.perf_counter()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Spacy Execution time: {execution_time:.2f} seconds\\n\\n\")\n",
        "\n",
        "print(\"spaCy Tokenization:\\n\")\n",
        "sms_spacy = lambda i: print(tokenized_sms_spacy[i])\n",
        "for i in range(4):\n",
        "    sms_spacy(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFQqAmaieg1p",
        "outputId": "bb2deb36-6004-4001-ec10-a87304326a87",
        "cellView": "form"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nltk Execution time: 1.00 seconds\n",
            "\n",
            "\n",
            "NLTK Tokenization:\n",
            "\n",
            "\n",
            "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n",
            "['Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...']\n",
            "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\"]\n",
            "['U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...']\n",
            "Spacy Execution time: 41.33 seconds\n",
            "\n",
            "\n",
            "spaCy Tokenization:\n",
            "\n",
            "['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'got', 'amore', 'wat', '...']\n",
            "['Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...']\n",
            "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', 'rate)T&C', \"'s\", 'apply', '08452810075over18', \"'s\"]\n",
            "['U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title lemmatizer part\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "\n",
        "# Tokenize and lemmatize using NLTK\n",
        "lemmatized_sms_nltk = []\n",
        "for sms in spam_text:\n",
        "    tokens = word_tokenize(sms)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
        "    lemmatized_sms_nltk.append(lemmatized_tokens)\n",
        "\n",
        "print(\"NLTK Lemmatization:\\n\")\n",
        "print(lemmatized_sms_nltk[0:5])\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "lemmatized_sms_spacy = [[token.lemma_ for token in NLP(sms)] for sms in spam_text]\n",
        "\n",
        "print(\"spaCy Lemmatization:\\n\")\n",
        "print(lemmatized_sms_spacy[0:5])\n",
        "print(\"\\n\\n\")\n",
        "# Time Complexity Analysis:\n",
        "# NLTK Lemmatization:\n",
        "# - Tokenization: O(m), where m is the number of characters.\n",
        "# - POS Tagging: O(n), where n is the number of tokens.\n",
        "# - Lemmatization: O(n), where n is the number of tokens.\n",
        "# Overall: O(m) + O(n) + O(n) = O(n), considering m ≈ n.\n",
        "\n",
        "# spaCy Lemmatization:\n",
        "# - Tokenization, POS Tagging, Lemmatization: O(n), where n is the number of tokens.\n",
        "# Overall: O(n)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "354hNbz1egzl",
        "outputId": "3661649c-c4ff-4113-f5ff-9583b963faee",
        "cellView": "form"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Lemmatization:\n",
            "\n",
            "[['Go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...'], ['Ok', 'lar', '...', 'Joking', 'wif', 'u', 'oni', '...'], ['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 'T', '&', 'C', \"'s\", 'apply', '08452810075over18', \"'s\"], ['U', 'dun', 'say', 'so', 'early', 'hor', '...', 'U', 'c', 'already', 'then', 'say', '...'], ['Nah', 'I', 'do', \"n't\", 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though']]\n",
            "\n",
            "\n",
            "\n",
            "spaCy Lemmatization:\n",
            "\n",
            "[['go', 'until', 'jurong', 'point', ',', 'crazy', '..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'Cine', 'there', 'get', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005', '.', 'text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', 'rate)T&C', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...'], ['nah', 'I', 'do', 'not', 'think', 'he', 'go', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though']]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title stemmer part\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "# Tokenize and stem using NLTK\n",
        "stemmed_sms_nltk = []\n",
        "for sms in spam_text:\n",
        "    tokens = word_tokenize(sms)\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    stemmed_sms_nltk.append(stemmed_tokens)\n",
        "\n",
        "print(\"NLTK Stemming:\\n\")\n",
        "print(stemmed_sms_nltk[0:5])\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Tokenize using spaCy and stem using NLTK\n",
        "stemmed_sms_spacy = []\n",
        "for sms in spam_text:\n",
        "    doc = NLP(sms)\n",
        "    stemmed_tokens = [stemmer.stem(token.text) for token in doc]\n",
        "    stemmed_sms_spacy.append(stemmed_tokens)\n",
        "\n",
        "print(\"spaCy (with NLTK Stemming):\\n\")\n",
        "print(stemmed_sms_spacy[0:5])\n",
        "print(\"\\n\\n\")\n",
        "# Time Complexity Analysis:\n",
        "# NLTK Stemming:\n",
        "# - Tokenization: O(m), where m is the number of characters.\n",
        "# - Stemming: O(n), where n is the number of tokens.\n",
        "# Overall: O(m) + O(n) = O(n), considering m ≈ n.\n",
        "\n",
        "# spaCy Stemming (using NLTK):\n",
        "# - Tokenization: O(m), where m is the number of characters.\n",
        "# - Stemming: O(n), where n is the number of tokens.\n",
        "# Overall: O(n)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQjmww79eykD",
        "outputId": "e728d68a-37aa-40a9-8dab-5169fd168276",
        "cellView": "form"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Stemming:\n",
            "\n",
            "[['go', 'until', 'jurong', 'point', ',', 'crazi', '..', 'avail', 'onli', 'in', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amor', 'wat', '...'], ['ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...'], ['free', 'entri', 'in', '2', 'a', 'wkli', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkt', '21st', 'may', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receiv', 'entri', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', 'c', \"'s\", 'appli', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'earli', 'hor', '...', 'u', 'c', 'alreadi', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goe', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though']]\n",
            "\n",
            "\n",
            "\n",
            "spaCy (with NLTK Stemming):\n",
            "\n",
            "[['go', 'until', 'jurong', 'point', ',', 'crazi', '..', 'avail', 'onli', 'in', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amor', 'wat', '...'], ['ok', 'lar', '...', 'joke', 'wif', 'u', 'oni', '...'], ['free', 'entri', 'in', '2', 'a', 'wkli', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkt', '21st', 'may', '2005', '.', 'text', 'fa', 'to', '87121', 'to', 'receiv', 'entri', 'question(std', 'txt', 'rate)t&c', \"'s\", 'appli', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'earli', 'hor', '...', 'u', 'c', 'alreadi', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goe', 'to', 'usf', ',', 'he', 'live', 'around', 'here', 'though']]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Stemming with nltk:\\n\\n\")\n",
        "analyze_array_data(stemmed_sms_nltk)\n",
        "print(\"\\n\")\n",
        "print(\"Stemming with spacy:\\n\\n\")\n",
        "analyze_array_data(stemmed_sms_spacy)\n"
      ],
      "metadata": {
        "id": "GsnVT1mTeygz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055a9962-58fe-42c9-dfbb-63a51fa5a717"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming with nltk:\n",
            "\n",
            "\n",
            "Total words: 104193\n",
            "Average words per message: 18.70\n",
            "Most common words: ., i, to, you, ,\n",
            "Unique words appearing only once: 4179\n",
            "\n",
            "\n",
            "Stemming with spacy:\n",
            "\n",
            "\n",
            "Total words: 103533\n",
            "Average words per message: 18.58\n",
            "Most common words: ., i, to, you, ,\n",
            "Unique words appearing only once: 4252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatized with nltk:\\n\\n\")\n",
        "analyze_array_data(lemmatized_sms_nltk)\n",
        "print(\"\\n\")\n",
        "print(\"Lemmatized with spacy:\\n\\n\")\n",
        "analyze_array_data(lemmatized_sms_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twpdGHNWeye8",
        "outputId": "95376c79-ce56-48f3-b472-9f48c4edab02"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized with nltk:\n",
            "\n",
            "\n",
            "Total words: 104193\n",
            "Average words per message: 18.70\n",
            "Most common words: ., be, to, I, you\n",
            "Unique words appearing only once: 5693\n",
            "\n",
            "\n",
            "Lemmatized with spacy:\n",
            "\n",
            "\n",
            "Total words: 103533\n",
            "Average words per message: 18.58\n",
            "Most common words: ., I, be, to, you\n",
            "Unique words appearing only once: 5359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title BeautifulSoup part\n",
        "websites = ['https://afgprogrammer.com/flutter/',\n",
        "            'https://huggingface.co/docs/transformers/model_doc/hubert']\n",
        "\n",
        "scrap_sentences = []\n",
        "\n",
        "for web in websites:\n",
        "    soup = BeautifulSoup(requests.get(web).text,'html.parser')\n",
        "    for p in soup.find_all('p'):\n",
        "        scrap_sentences.append(p.text)\n",
        "\n",
        "\n",
        "print(\"Scraped Sentences:\\n\")\n",
        "print(scrap_sentences[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU3LUq57eyZ_",
        "outputId": "f8ac2902-6c95-4000-8f63-eada764a61c0",
        "cellView": "form"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Sentences:\n",
            "\n",
            "['Search in 100+ Flutter Examples ', 'Day 64 of Flutter 100 days of code.', 'Day 63 of Flutter 100 days of code.', 'Day 62 of Flutter 100 days of code.', 'Day 61 of Flutter 100 days of code.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "printListsAfterActivation(scrap_sentences)\n",
        "token_stem_limmatizeing(scrap_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjTUwM26lY1B",
        "outputId": "5375bcf9-0d26-4a57-acee-b0a3f15861d9"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Sentences:\n",
            "[['Search', 'in', '100+', 'Flutter', 'Examples'], ['Day', '64', 'of', 'Flutter', '100', 'days', 'of', 'code', '.'], ['Day', '63', 'of', 'Flutter', '100', 'days', 'of', 'code', '.'], ['Day', '62', 'of', 'Flutter', '100', 'days', 'of', 'code', '.'], ['Day', '61', 'of', 'Flutter', '100', 'days', 'of', 'code', '.']]\n",
            "\n",
            "Lemmatized Sentences:\n",
            "[['search', 'in', '100', '+', 'flutter', 'example'], ['day', '64', 'of', 'Flutter', '100', 'day', 'of', 'code', '.'], ['day', '63', 'of', 'Flutter', '100', 'day', 'of', 'code', '.'], ['day', '62', 'of', 'Flutter', '100', 'day', 'of', 'code', '.'], ['day', '61', 'of', 'Flutter', '100', 'day', 'of', 'code', '.']]\n",
            "\n",
            "Stemmed Sentences:\n",
            "[['search', 'in', '100+', 'flutter', 'exampl'], ['day', '64', 'of', 'flutter', '100', 'day', 'of', 'code', '.'], ['day', '63', 'of', 'flutter', '100', 'day', 'of', 'code', '.'], ['day', '62', 'of', 'flutter', '100', 'day', 'of', 'code', '.'], ['day', '61', 'of', 'flutter', '100', 'day', 'of', 'code', '.']]\n",
            "\n",
            "\n",
            "\n",
            "Original Text Statistics:\n",
            "Total words: 5736\n",
            "Average words per message: 22.32\n",
            "Most common words: ., ,, the, of, (\n",
            "Unique words appearing only once: 296\n",
            "\n",
            "Lemmatized Text Statistics:\n",
            "Total words: 6290\n",
            "Average words per message: 24.47\n",
            "Most common words: ., the, ,, of, (\n",
            "Unique words appearing only once: 257\n",
            "\n",
            "Stemmed Text Statistics:\n",
            "Total words: 5736\n",
            "Average words per message: 22.32\n",
            "Most common words: ., the, ,, of, (\n",
            "Unique words appearing only once: 252\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = open(\"/content/whatsup.txt\", 'r')\n",
        "chat_text = chat.readlines()\n",
        "print(chat_text[0:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDCqmwnUeyYK",
        "outputId": "814eb77e-bfae-4fb3-df39-e27eaa668c3f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['הוא מוגש עם המסמך הסופי בתאריך שהוגדר לכם.\\n', 'בדמו או בספר הפרויקט?\\n', 'יש שם 2 שלבים\\n', ' למיטב זכרוני, מועד הגשת הטיוטה הוא בשביל להגיע מוכנים לתערוכה, כלומר - עד אליו.\\n', 'אחרי כן יהיה זמן לתיקונים קטנים מעטים במידת הצורך.\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "printListsAfterActivation(chat_text)\n",
        "token_stem_limmatizeing(chat_text)"
      ],
      "metadata": {
        "id": "p2b7azUZeyWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d250b49-1108-4726-cd52-67dc1f4a2d4f"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Sentences:\n",
            "[['הוא', 'מוגש', 'עם', 'המסמך', 'הסופי', 'בתאריך', 'שהוגדר', 'לכם', '.'], ['בדמו', 'או', 'בספר', 'הפרויקט', '?'], ['יש', 'שם', '2', 'שלבים'], ['למיטב', 'זכרוני', ',', 'מועד', 'הגשת', 'הטיוטה', 'הוא', 'בשביל', 'להגיע', 'מוכנים', 'לתערוכה', ',', 'כלומר', '-', 'עד', 'אליו', '.'], ['אחרי', 'כן', 'יהיה', 'זמן', 'לתיקונים', 'קטנים', 'מעטים', 'במידת', 'הצורך', '.']]\n",
            "\n",
            "Lemmatized Sentences:\n",
            "[['הוא', 'מוגש', 'עם', 'המסמך', 'הסופי', 'בתאריך', 'שהוגדר', 'לכם', '.'], ['בדמו', 'או', 'בספר', 'הפרויקט', '?'], ['יש', 'שם', '2', 'שלבים'], ['למיטב', 'זכרוני', ',', 'מועד', 'הגשת', 'הטיוטה', 'הוא', 'בשביל', 'להגיע', 'מוכנים', 'לתערוכה', ',', 'כלומר', '-', 'עד', 'אליו', '.'], ['אחרי', 'כן', 'יהיה', 'זמן', 'לתיקונים', 'קטנים', 'מעטים', 'במידת', 'הצורך', '.']]\n",
            "\n",
            "Stemmed Sentences:\n",
            "[['הוא', 'מוגש', 'עם', 'המסמך', 'הסופי', 'בתאריך', 'שהוגדר', 'לכם', '.'], ['בדמו', 'או', 'בספר', 'הפרויקט', '?'], ['יש', 'שם', '2', 'שלבים'], ['למיטב', 'זכרוני', ',', 'מועד', 'הגשת', 'הטיוטה', 'הוא', 'בשביל', 'להגיע', 'מוכנים', 'לתערוכה', ',', 'כלומר', '-', 'עד', 'אליו', '.'], ['אחרי', 'כן', 'יהיה', 'זמן', 'לתיקונים', 'קטנים', 'מעטים', 'במידת', 'הצורך', '.']]\n",
            "\n",
            "\n",
            "\n",
            "Original Text Statistics:\n",
            "Total words: 1010\n",
            "Average words per message: 7.54\n",
            "Most common words: ?, של, את, יש, לא\n",
            "Unique words appearing only once: 438\n",
            "\n",
            "Lemmatized Text Statistics:\n",
            "Total words: 1023\n",
            "Average words per message: 7.63\n",
            "Most common words: ?, של, את, יש, לא\n",
            "Unique words appearing only once: 438\n",
            "\n",
            "Stemmed Text Statistics:\n",
            "Total words: 1010\n",
            "Average words per message: 7.54\n",
            "Most common words: ?, של, את, יש, לא\n",
            "Unique words appearing only once: 436\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/NlpTask1_Dori_Shlomi.ipynb"
      ],
      "metadata": {
        "id": "e14r0br9ut_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}