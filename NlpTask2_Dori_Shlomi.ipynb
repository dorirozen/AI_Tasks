{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzw1irJViM6c2YX5kNdvUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorirozen/AI_Tasks/blob/main/NlpTask2_Dori_Shlomi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Creating the corpus from prev task\n",
        "!pip install beautifulsoup4\n",
        "import nltk,time,spacy,csv,requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "#@title BeautifulSoup part\n",
        "websites = ['https://afgprogrammer.com/flutter/','https://huggingface.co/docs/transformers/model_doc/hubert']\n",
        "\n",
        "scrap_sentences = []\n",
        "\n",
        "for web in websites:\n",
        "    soup = BeautifulSoup(requests.get(web).text,'html.parser')\n",
        "    for p in soup.find_all('p'):\n",
        "        scrap_sentences.append(p.text)\n",
        "\n",
        "\n",
        "print(\"Scraped Sentences:\\n\")\n",
        "print(scrap_sentences[0:5])\n",
        "\n",
        "# Save the corpus to a text file\n",
        "with open('corpus.txt', 'w', encoding='utf-8') as file:\n",
        "    for sentence in scrap_sentences:\n",
        "        file.write(sentence + '\\n')\n",
        "\n",
        "print(\"Corpus saved to corpus.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKyyo-Mm3pT5",
        "outputId": "b8a7b60c-b4e6-48c2-d891-2b6c3ba50464"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Scraped Sentences:\n",
            "\n",
            "['Search in 100+ Flutter Examples ', 'Day 64 of Flutter 100 days of code.', 'Day 63 of Flutter 100 days of code.', 'Day 62 of Flutter 100 days of code.', 'Day 61 of Flutter 100 days of code.']\n",
            "Corpus saved to corpus.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n"
      ],
      "metadata": {
        "id": "ckAeWmaVXq7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy , nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer,RegexpTokenizer,sent_tokenize, word_tokenize\n",
        "from nltk.grammar import CFG, Nonterminal\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer,WordNetLemmatizer\n",
        "from pandas import unique\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "3v165BD5yJFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0aca861-f219-482e-84e3-23035170cf9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the corpus we created in homework 1"
      ],
      "metadata": {
        "id": "xX-GI8HEXtiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "with open(\"corpus.txt\",\"r\", encoding = \"ISO-8859-1\") as file:\n",
        "  for s in file.readlines():\n",
        "    sentences.append(s.replace(\"\\n\",\"\"))\n",
        "\n",
        "print(f\"sentences size: {len(sentences)}\")\n"
      ],
      "metadata": {
        "id": "YRwLbFUdXi6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5247539-2789-4af8-d86c-72faeba19d80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentences size: 463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply tokenization:\n",
        "  - White space tokenizer\n",
        "  - Regex tokenizer\n",
        "  - Word tokenizer\n",
        "  - Sentence tokenizer"
      ],
      "metadata": {
        "id": "sWo8NHz4aEZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\" This function return list of tokens \"\"\"\n",
        "def tokenizer(sentences:list[str],tokenize,limit=-1):\n",
        "  tokens = []\n",
        "  for s in sentences[:limit]:\n",
        "    tokens.extend(tokenize(s))\n",
        "  return list(unique(tokens))\n",
        "\n",
        "print(f\"The sentence:\\n{sentences[0]}\\nTokens:\")\n",
        "\n",
        "#White space tokenizer\n",
        "wstk = WhitespaceTokenizer()\n",
        "print(f\"White space tokenizer: {tokenizer(sentences,wstk.tokenize,limit=5)}\")\n",
        "\n",
        "#Regex tokenizer\n",
        "rgtk = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "print(f\"Regex tokenizer: {tokenizer(sentences,rgtk.tokenize,limit=5)}\")\n",
        "\n",
        "#Word tokenizer\n",
        "print(f\"Word tokenizer: {tokenizer(sentences,word_tokenize,limit=5)}\")\n",
        "\n",
        "#Sentence tokenizer\n",
        "print(f\"Sentence tokenizer: {tokenizer(sentences,sent_tokenize,limit=5)}\")"
      ],
      "metadata": {
        "id": "fJG0zTpNXi4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b516d49a-ada5-4ecc-fe14-8c226cb8a1ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence:\n",
            "Search in 100+ Flutter Examples \n",
            "Tokens:\n",
            "White space tokenizer: ['Search', 'in', '100+', 'Flutter', 'Examples', 'Day', '64', 'of', '100', 'days', 'code.', '63', '62', '61']\n",
            "Regex tokenizer: ['Search', 'in', '100', '+', 'Flutter', 'Examples', 'Day', '64', 'of', 'days', 'code', '.', '63', '62', '61']\n",
            "Word tokenizer: ['Search', 'in', '100+', 'Flutter', 'Examples', 'Day', '64', 'of', '100', 'days', 'code', '.', '63', '62', '61']\n",
            "Sentence tokenizer: ['Search in 100+ Flutter Examples', 'Day 64 of Flutter 100 days of code.', 'Day 63 of Flutter 100 days of code.', 'Day 62 of Flutter 100 days of code.', 'Day 61 of Flutter 100 days of code.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Normalization:\n",
        "- Stemming\n",
        "- Lemmatization"
      ],
      "metadata": {
        "id": "b1HTHR1UZ7Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example sentences (it look better then just passing the first sentences..)\n",
        "sentencesNorm = [ # choose 5 that make sense..\n",
        "    'A Flutter staggered grid view which supports multiple columns with rows of varying sizes.',\n",
        "    'Use dynamic and beautiful card view pagers to help you create great apps.',\n",
        "    'Yet another bottom navigation bar with a few key promises.',\n",
        "    'Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan',\n",
        "    'Salakhutdinov, Abdelrahman Mohamed.'\n",
        "]\n",
        "\n",
        "\n",
        "# Normalization function\n",
        "def normalization(tokens, normalizer, limit=-1):\n",
        "    return [normalizer(s) for s in tokens[:limit]]\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenizer(sentences, tokenizer):\n",
        "    return [token for sentence in sentences for token in tokenizer(sentence)]\n",
        "\n",
        "# Stemming and lemmatization\n",
        "def process_normalization(sentences, normalizer, limit=15):\n",
        "    tokens = tokenizer(sentences, word_tokenize)\n",
        "    return normalization(tokens, normalizer, limit)\n",
        "\n",
        "# Initialize Spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Normalizers\n",
        "normalizers = {\n",
        "    \"Porter Stemming\": PorterStemmer().stem,\n",
        "    \"Lancaster Stemming\": LancasterStemmer().stem,\n",
        "    \"Snowball Stemming\": SnowballStemmer(language='english').stem,\n",
        "    \"NLTK Lemmatization\": WordNetLemmatizer().lemmatize,\n",
        "    \"Spacy Lemmatization\": lambda s: [token.lemma_ for token in nlp(s)]\n",
        "}\n",
        "\n",
        "# Apply normalizers and print results\n",
        "for name, normalizer in normalizers.items():\n",
        "    if name == \"Spacy Lemmatization\":\n",
        "        result = [normalizer(sentence) for sentence in sentencesNorm[:5]]\n",
        "    else:\n",
        "        result = process_normalization(sentencesNorm, normalizer)\n",
        "    print(f\"{name}: {result[:15]}\")\n"
      ],
      "metadata": {
        "id": "K6Q2xGzXXi1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1244a90-7a25-4884-feb5-a31fa4d81511"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemming: ['a', 'flutter', 'stagger', 'grid', 'view', 'which', 'support', 'multipl', 'column', 'with', 'row', 'of', 'vari', 'size', '.']\n",
            "Lancaster Stemming: ['a', 'flut', 'stag', 'grid', 'view', 'which', 'support', 'multipl', 'column', 'with', 'row', 'of', 'vary', 'siz', '.']\n",
            "Snowball Stemming: ['a', 'flutter', 'stagger', 'grid', 'view', 'which', 'support', 'multipl', 'column', 'with', 'row', 'of', 'vari', 'size', '.']\n",
            "NLTK Lemmatization: ['A', 'Flutter', 'staggered', 'grid', 'view', 'which', 'support', 'multiple', 'column', 'with', 'row', 'of', 'varying', 'size', '.']\n",
            "Spacy Lemmatization: [['a', 'Flutter', 'stagger', 'grid', 'view', 'which', 'support', 'multiple', 'column', 'with', 'row', 'of', 'vary', 'size', '.'], ['use', 'dynamic', 'and', 'beautiful', 'card', 'view', 'pager', 'to', 'help', 'you', 'create', 'great', 'app', '.'], ['yet', 'another', 'bottom', 'navigation', 'bar', 'with', 'a', 'few', 'key', 'promise', '.'], ['Hubert', 'be', 'propose', 'in', 'hubert', ':', 'self', '-', 'supervise', 'Speech', 'Representation', 'Learning', 'by', 'Masked', 'Prediction', 'of', 'Hidden', 'Units', 'by', 'Wei', '-', 'Ning', 'Hsu', ',', 'Benjamin', 'Bolte', ',', 'Yao', '-', 'Hung', 'Hubert', 'Tsai', ',', 'Kushal', 'Lakhotia', ',', 'Ruslan'], ['Salakhutdinov', ',', 'Abdelrahman', 'Mohamed', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop Words: conjunctions and articles"
      ],
      "metadata": {
        "id": "fctwzQLQX7Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQTZ0dDq6K5K",
        "outputId": "7c7e2faa-07e1-4cd4-980e-c830a2791adb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words_filtered = [word for word in words if word.lower() not in stop_words]\n",
        "    text_filtered = ' '.join(words_filtered)\n",
        "    return text_filtered\n",
        "\n",
        "print(f\"Source: {sentences[0]}\")\n",
        "print(f\"Remove Stop Words: {remove_stopwords(sentences[0])}\")\n"
      ],
      "metadata": {
        "id": "RezNdQLPXiyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6322e2-2244-4b21-cf0a-99d1d4408ee3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source: Search in 100+ Flutter Examples \n",
            "Remove Stop Words: Search 100+ Flutter Examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply feature extraction by the following algorithms:\n",
        "- BOW\n",
        "- TF-IDF\n",
        "- Word embedding by WORD2VEC"
      ],
      "metadata": {
        "id": "5MaCUdqOX8vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BOW\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(sentences)\n",
        "print(f\"Matrix shape: {len(bow_matrix.toarray())}x{len(bow_matrix.toarray()[0])}\\n\")\n",
        "\n",
        "print(f\"--- BOW ---\")\n",
        "print(bow_matrix.toarray()[0][110:120])\n",
        "print(bow_vectorizer.get_feature_names_out()[110:120])\n",
        "\n",
        "# TF-IDF\n",
        "print(f\"\\n\\n--- TF-IDF ---\")\n",
        "tf_idf_vectorizer = TfidfVectorizer()\n",
        "print(tf_idf_vectorizer.fit_transform(sentences)[0])\n",
        "print(tf_idf_vectorizer.get_feature_names_out()[110:120])\n",
        "\n"
      ],
      "metadata": {
        "id": "xVyIvP8RXiwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6a5fc3-7b4d-433a-8862-cb37c0393708"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix shape: 463x673\n",
            "\n",
            "--- BOW ---\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "['benefit' 'benjamin' 'bert' 'better' 'between' 'bolte' 'bool'\n",
            " 'bos_token_id' 'bottom' 'bouncing']\n",
            "\n",
            "\n",
            "--- TF-IDF ---\n",
            "  (0, 207)\t0.5859543157170254\n",
            "  (0, 226)\t0.2933855212089265\n",
            "  (0, 4)\t0.3428966492317968\n",
            "  (0, 281)\t0.3311523873925078\n",
            "  (0, 506)\t0.5859543157170254\n",
            "['benefit' 'benjamin' 'bert' 'better' 'between' 'bolte' 'bool'\n",
            " 'bos_token_id' 'bottom' 'bouncing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Word embedding by WORD2VEC\n",
        "corpus = []\n",
        "for s in sentences:\n",
        "  corpus.append(tokenizer([s],word_tokenize,limit=1))\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=corpus,   # The corpus to train the model on\n",
        "    vector_size=100,     # The size of the word vectors to be learned\n",
        "    window=5,              # The size of the window of words to be considered\n",
        "    min_count=1,           # The minimum frequency required for a word to be included in the vocabulary\n",
        "    sg=0,                  # 0 for CBOW, 1 for skip-gram\n",
        "    negative=5,            # The number of negative samples to use for negative sampling\n",
        "    ns_exponent=0.75,      # The exponent used to shape the negative sampling distribution\n",
        "    alpha=0.03,            # The initial learning rate\n",
        "    min_alpha=0.0007,      # The minimum learning rate to which the learning rate will be linearly reduced\n",
        "    epochs=30,             # The number of epochs (iterations) over the corpus\n",
        "    workers=4,             # The number of worker threads to use for training the model\n",
        "    seed=42,               # The seed for the random number generator\n",
        "    max_vocab_size=None    # The maximum vocabulary size (None means no limit)\n",
        ")\n",
        "\n",
        "word_to_check = 'Flutter'\n",
        "\n",
        "if word_to_check in model.wv:\n",
        "    vector = model.wv[word_to_check]\n",
        "    # Find the most similar words to a given word\n",
        "    similar_words = model.wv.most_similar(word_to_check)\n",
        "\n",
        "    # Print the vector and similar words\n",
        "    print(f\"Vector for '{word_to_check}':\", vector)\n",
        "    print(f\"Most similar words to '{word_to_check}':\", similar_words)\n",
        "else:\n",
        "    print(f\"Word '{word_to_check}' not found in vocabulary.\")"
      ],
      "metadata": {
        "id": "rCHo9Ab2XivO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45967749-2b89-4e02-ea11-3713623a6346"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'Flutter': [-0.14729781  0.29750034 -0.38956866  0.40070105  0.12812424  0.4901716\n",
            "  0.11166948  0.7940336  -0.3921287   0.35432923 -0.42261598  0.8413246\n",
            "  0.13609315 -0.02202416 -0.08763188 -0.17329317 -0.24687956 -0.2766637\n",
            " -0.0833912  -0.10908526  0.40702906  0.28916106  0.47542235  0.24836221\n",
            " -0.07753623  0.54155904 -0.5260723   0.04159054 -0.32424164 -0.3875142\n",
            "  0.01231295 -0.17406069 -0.1954934   0.6838543   0.12853292 -0.4455833\n",
            " -0.60480535 -0.73664635 -0.16314745  0.46314356  0.16573843  0.23868641\n",
            "  0.4834183  -0.07753997  0.26256952  0.57499176  0.05003868  0.6256608\n",
            "  0.5687501   0.2560088  -0.4048323  -0.5816231   0.27039713  0.1509341\n",
            "  0.1913862  -0.3174618   0.28707162 -0.1933461  -0.2852275   0.3599017\n",
            " -0.65164196 -0.01407406 -0.09053874  0.22271201 -0.05659182 -0.24223958\n",
            " -0.01063277 -0.6355571   0.13675553 -0.19490103 -0.24847013 -0.1461057\n",
            "  0.06155507 -0.2457478   0.3061659   0.1618551   0.04067988  0.41544604\n",
            " -0.08851941  0.13894758  0.50989383 -0.20527844 -0.21977188  0.27640063\n",
            "  0.19193332 -0.33014527 -0.1957045  -0.3402645   0.11078883  0.04434227\n",
            " -0.6327324   0.35775977  0.17793307 -0.45211428  0.4761154  -0.28674933\n",
            " -0.21846542 -0.00823481 -0.73787284 -0.32473412]\n",
            "Most similar words to 'Flutter': [('Day', 0.9900573492050171), ('100', 0.9875499606132507), ('days', 0.9874377250671387), ('code', 0.9872047305107117), ('50', 0.9812667369842529), ('35', 0.9789412021636963), ('45', 0.9768866896629333), ('56', 0.9754147529602051), ('63', 0.9740689992904663), ('40', 0.9716053605079651)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is GloVe?  \n",
        "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm developed by researchers at Stanford for obtaining vector representations for words. The main idea behind GloVe is to capture the meaning of words by looking at their co-occurrence statistics in a large corpus. GloVe vectors represent words in a high-dimensional space where the distance and direction between words can capture semantic relationships."
      ],
      "metadata": {
        "id": "Ps1wTt2-ELjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Ensure you have the GloVe file downloaded\n",
        "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "zipfilename = url.rsplit('/', 1)[-1]\n",
        "filename = zipfilename.rsplit('.', 1)[0]\n",
        "\n",
        "print(\"Downloading GloVe embeddings...\")\n",
        "urllib.request.urlretrieve(url, zipfilename)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "print(\"Extracting GloVe embeddings...\")\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "print(\"Extraction complete.\")\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_model(glove_file):\n",
        "    print(\"Loading GloVe model...\")\n",
        "    model = {}\n",
        "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array([float(val) for val in split_line[1:]])\n",
        "            model[word] = embedding\n",
        "    print(\"Done.\", len(model), \" words loaded!\")\n",
        "    return model\n",
        "\n",
        "# Path to GloVe file\n",
        "glove_file = \"glove.6B.50d.txt\"  # Make sure this file is in your working directory\n",
        "\n",
        "# Load the GloVe model\n",
        "glove_model = load_glove_model(glove_file)\n",
        "\n",
        "# Example sentences\n",
        "sentences = [\n",
        "    'A Flutter staggered grid view which supports multiple columns with rows of varying sizes.',\n",
        "    'Use dynamic and beautiful card view pagers to help you create great apps.',\n",
        "    'Yet another bottom navigation bar with a few key promises.',\n",
        "    'Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan',\n",
        "    'Salakhutdinov, Abdelrahman Mohamed.'\n",
        "]\n",
        "\n",
        "# Function to convert sentences to GloVe embeddings\n",
        "def sentence_to_glove(sentence, glove_model):\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "    vectors = [glove_model[token] for token in tokens if token in glove_model]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(50)  # Return a zero vector if no tokens are found\n",
        "\n",
        "# Compute GloVe embeddings for sentences\n",
        "glove_embeddings = [sentence_to_glove(sentence, glove_model) for sentence in sentences]\n",
        "\n",
        "# Print the embeddings\n",
        "for i, embedding in enumerate(glove_embeddings):\n",
        "    print(f\"Sentence {i+1} GloVe embedding: {embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_r9wLUMFzUi",
        "outputId": "0b099139-f57b-4e1f-9e51-824c5dfe09bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings...\n",
            "Download complete.\n",
            "Extracting GloVe embeddings...\n",
            "Extraction complete.\n",
            "Loading GloVe model...\n",
            "Done. 400000  words loaded!\n",
            "Sentence 1 GloVe embedding: [ 0.3484296   0.3851346   0.1631234  -0.08282333  0.1819038   0.52050847\n",
            " -0.00614613 -0.45836733 -0.1030844  -0.25582107 -0.1319028  -0.31361333\n",
            " -0.1020254   0.16592813 -0.13693787 -0.03395488 -0.19639703 -0.13824147\n",
            " -0.09832622 -0.7163274   0.02562753 -0.2026846   0.18284467  0.07186505\n",
            " -0.09771815 -0.570934   -0.201538    0.25061093  0.12621231 -0.0205856\n",
            "  2.71114933  0.06023756  0.00571573 -0.35129447  0.03799913  0.15220443\n",
            "  0.07142327  0.06880487 -0.23841413  0.09578153  0.07572035 -0.0519618\n",
            " -0.04808693  0.3564852  -0.249343    0.04569329  0.2498818  -0.0949004\n",
            " -0.1268876  -0.37642407]\n",
            "Sentence 2 GloVe embedding: [ 3.52329364e-01  2.07038857e-01  2.26760000e-01  1.04433357e-01\n",
            "  3.48035857e-01  4.86328571e-02 -3.26107500e-01 -4.52099271e-01\n",
            " -3.00410714e-02  3.56521500e-01  2.74543571e-02  3.20700300e-01\n",
            "  7.16107429e-02 -8.55755000e-02  1.02493286e-01  2.60634714e-01\n",
            "  7.70135714e-03 -2.09741429e-02  1.24057500e-01 -5.86000714e-01\n",
            "  4.01846357e-02  3.15576429e-02 -1.00948714e-01  9.49768571e-02\n",
            "  4.03779143e-01 -1.12713000e+00 -4.13715500e-01 -2.58487929e-01\n",
            "  4.67541221e-01 -5.64293143e-01  2.95717143e+00  3.88039529e-01\n",
            " -2.69944929e-01 -3.97938286e-01  9.85378571e-04  2.01439286e-01\n",
            " -5.80848571e-02  2.16398214e-01 -2.20696429e-01 -2.92717629e-01\n",
            "  2.42641166e-01  6.30607143e-02 -1.41795714e-02  2.79901857e-01\n",
            "  1.13297214e-01  2.22176143e-01  1.47425643e-01 -1.99499071e-01\n",
            " -1.27559286e-02  6.92462214e-02]\n",
            "Sentence 3 GloVe embedding: [ 1.88216091e-01  2.70677182e-01  8.26272727e-04  3.20154545e-02\n",
            "  2.83323636e-01  1.68087727e-01 -4.84839273e-01 -3.49863636e-02\n",
            " -4.84513636e-02 -6.81909091e-04 -1.60397182e-01  5.63601818e-02\n",
            " -3.10256091e-01  2.44699545e-01  2.69047273e-02  5.61627273e-02\n",
            "  6.34852727e-02 -1.83174273e-01 -2.63921909e-01 -4.86931273e-01\n",
            "  1.74626000e-01  9.21201818e-02  6.67363636e-03 -3.04788182e-02\n",
            "  3.33230909e-02 -1.42303273e+00 -2.15766427e-01  2.59728545e-01\n",
            "  3.25434989e-01 -1.18354636e-01  3.15684545e+00  1.39540364e-02\n",
            " -2.62019455e-01 -1.04424943e-01  2.41669909e-01  7.71427273e-02\n",
            "  4.09209091e-02  2.07300909e-01  6.28731818e-02 -2.45303455e-01\n",
            " -7.06760736e-03  1.25605418e-01 -5.41316364e-02  1.00890073e-01\n",
            " -1.34878182e-01  8.15030000e-02  4.14311818e-02  4.08610909e-02\n",
            "  2.30930273e-01 -1.56363909e-01]\n",
            "Sentence 4 GloVe embedding: [ 0.15348415  0.22695074 -0.04504164  0.22094215  0.21460707  0.34446878\n",
            " -0.2488867  -0.2968017  -0.48035956 -0.25252356  0.08131068  0.10912344\n",
            " -0.16673719 -0.06027359  0.02577578 -0.02868519 -0.03861367 -0.14863789\n",
            " -0.23328541 -0.03563463  0.09472748  0.12725663  0.08018822 -0.28340526\n",
            "  0.18323519 -0.92892681 -0.04876207 -0.17049904 -0.30982837  0.08271348\n",
            "  1.94741    -0.17112947 -0.1961783  -0.30947478  0.0097147  -0.1320751\n",
            "  0.04151744  0.18320767  0.08450833  0.25563381  0.05168033  0.32525984\n",
            " -0.02441233 -0.26002211  0.04566622 -0.12240674 -0.26588952 -0.13562915\n",
            "  0.22780493  0.10215778]\n",
            "Sentence 5 GloVe embedding: [ 0.08805525  0.12641775 -0.158195    0.4100475   0.6959925  -0.498145\n",
            " -0.12294575 -0.2246625   0.0946575  -0.03034175  0.4503225   0.359039\n",
            " -0.58758     0.0246625   0.4997075   0.22444525 -0.15855675 -0.21783975\n",
            "  0.06498     0.3308525  -0.141115    0.47514625  0.666505   -0.02931625\n",
            " -0.296992   -0.7802925   0.2158625  -0.05898075 -0.15020275  0.604785\n",
            "  1.7886875  -0.0775354  -0.37330075  0.1331875   0.23198975  0.1569675\n",
            "  0.08382     0.291735    0.247265    0.82321075  0.10964108  0.72952175\n",
            " -0.10093775 -0.1413375   0.204765   -0.0810225  -0.65857075 -0.09419375\n",
            "  0.375671   -0.1912425 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation and Results :  \n",
        "Loading GloVe Model: We load the pre-trained GloVe embeddings from a file.\n",
        "\n",
        "This file contains word vectors for many English words, each represented as a 50-dimensional vector.\n",
        "\n",
        "Tokenizing Sentences: Each sentence is tokenized into words.\n",
        "Mapping Tokens to GloVe Vectors: For each token in the sentence, we find the corresponding GloVe vector.\n",
        "\n",
        "Computing Sentence Embeddings: We compute the sentence embedding by averaging the GloVe vectors of the words in the sentence. If no words in the sentence are found in the GloVe model, we return a zero vector.\n",
        "\n",
        "The GloVe embeddings for each sentence are printed as high-dimensional vectors. These embeddings can be used for various NLP tasks such as clustering, classification, and similarity computations.\n",
        "\n",
        "The results (GloVe embeddings) capture the semantic meaning of the sentences, allowing us to use these vectors for more advanced NLP applications. For example, similar sentences will have embeddings that are close to each other in the vector space."
      ],
      "metadata": {
        "id": "xv-zdLt3FhMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select 5 sentences and apply to them Tagging by CYK algorithm\n"
      ],
      "metadata": {
        "id": "kuipA-cXYA77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"She sells sea shells by the sea shore.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"A fox jumped over the rat.\",\n",
        "    \"A dog jumped over the sea shore.\",\n",
        "]\n",
        "sentences"
      ],
      "metadata": {
        "id": "Rb-bgFL1Xiol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da7f216-089a-44b3-9583-c8686410b8ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['She sells sea shells by the sea shore.',\n",
              " 'The quick brown fox jumps over the lazy dog.',\n",
              " 'A fox jumped over the rat.',\n",
              " 'A dog jumped over the sea shore.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CYK PART\n",
        "1 sentence manully for :\n",
        "The cat sat on the mat."
      ],
      "metadata": {
        "id": "dxNKd-AP6L9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "H42Uyc9G6MfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CYK with code (good version)\n",
        "import re\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    return re.sub(r'[^\\w\\s]', '', sentence).lower()\n",
        "\n",
        "def cyk_parse(sentence, grammar):\n",
        "    words = sentence.split()\n",
        "    n = len(words)\n",
        "    table = [[set() for _ in range(n)] for _ in range(n)]\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        for lhs, rhs_list in grammar.items():\n",
        "            for rhs in rhs_list:\n",
        "                if isinstance(rhs, str) and rhs.lower() == word.lower():\n",
        "                    table[i][i].add(lhs)\n",
        "\n",
        "    for length in range(2, n + 1):\n",
        "        for i in range(n - length + 1):\n",
        "            j = i + length - 1\n",
        "            for k in range(i, j):\n",
        "                for lhs, rhs_list in grammar.items():\n",
        "                    for rhs in rhs_list:\n",
        "                        if isinstance(rhs, list) and len(rhs) == 2:\n",
        "                            B, C = rhs\n",
        "                            if B in table[i][k] and C in table[k+1][j]:\n",
        "                                table[i][j].add(lhs)\n",
        "\n",
        "    return table\n",
        "\n",
        "def print_cyk_table(table, words):\n",
        "    n = len(words)\n",
        "    print(f\"{'':6}\", end='')\n",
        "    for i, word in enumerate(words):\n",
        "        print(f\"{i+1:^12}\", end='')\n",
        "    print()\n",
        "\n",
        "    for i in range(n):\n",
        "        print(f\"{i+1:<6}\", end='')\n",
        "        for j in range(n):\n",
        "            if j < i:\n",
        "                print(f\"{'':12}\", end='')\n",
        "            else:\n",
        "                cell_content = ', '.join(sorted(table[i][j])) if table[i][j] else ''\n",
        "                print(f\"{cell_content:^12}\", end='')\n",
        "        print()\n",
        "\n",
        "grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'NP': [['Det', 'N']],\n",
        "    'VP': [['V', 'PP']],\n",
        "    'PP': [['Prep', 'NP']],\n",
        "    'Det': ['the', 'a'],\n",
        "    'N': ['cat', 'mat', 'dog', 'fox', 'rat'],\n",
        "    'V': ['sat', 'jumped'],\n",
        "    'Prep': ['on', 'over'],\n",
        "}\n",
        "\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"\\nSentence {i}:\")\n",
        "    print(sentence)\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    words = preprocessed_sentence.split()\n",
        "    result = cyk_parse(preprocessed_sentence, grammar)\n",
        "    print_cyk_table(result, words)\n",
        "    print(f\"\\nThe sentence is {'valid' if 'S' in result[0][len(words)-1] else 'invalid'} according to the grammar.\")\n",
        "    print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "HdZm4MJOXil8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d6bac8-9f48-4432-de2d-c275a31b9f96"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1:\n",
            "She sells sea shells by the sea shore.\n",
            "           1           2           3           4           5           6           7           8      \n",
            "1                                                                                                     \n",
            "2                                                                                                     \n",
            "3                                                                                                     \n",
            "4                                                                                                     \n",
            "5                                                                                                     \n",
            "6                                                                     Det                             \n",
            "7                                                                                                     \n",
            "8                                                                                                     \n",
            "\n",
            "The sentence is invalid according to the grammar.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence 2:\n",
            "The quick brown fox jumps over the lazy dog.\n",
            "           1           2           3           4           5           6           7           8           9      \n",
            "1         Det                                                                                                     \n",
            "2                                                                                                                 \n",
            "3                                                                                                                 \n",
            "4                                              N                                                                  \n",
            "5                                                                                                                 \n",
            "6                                                                     Prep                                        \n",
            "7                                                                                 Det                             \n",
            "8                                                                                                                 \n",
            "9                                                                                                          N      \n",
            "\n",
            "The sentence is invalid according to the grammar.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence 3:\n",
            "A fox jumped over the rat.\n",
            "           1           2           3           4           5           6      \n",
            "1         Det          NP                                              S      \n",
            "2                      N                                                      \n",
            "3                                  V                                   VP     \n",
            "4                                             Prep                     PP     \n",
            "5                                                         Det          NP     \n",
            "6                                                                      N      \n",
            "\n",
            "The sentence is valid according to the grammar.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Sentence 4:\n",
            "A dog jumped over the sea shore.\n",
            "           1           2           3           4           5           6           7      \n",
            "1         Det          NP                                                                 \n",
            "2                      N                                                                  \n",
            "3                                  V                                                      \n",
            "4                                             Prep                                        \n",
            "5                                                         Det                             \n",
            "6                                                                                         \n",
            "7                                                                                         \n",
            "\n",
            "The sentence is invalid according to the grammar.\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/NlpTask2_Dori_Shlomi.ipynb"
      ],
      "metadata": {
        "id": "L59XfMPB8NXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffcde7fb-de47-4ca0-9cb0-73cbb2ee3ab4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/NlpTask2_Dori_Shlomi.ipynb to html\n",
            "[NbConvertApp] Writing 681219 bytes to /content/NlpTask2_Dori_Shlomi.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}